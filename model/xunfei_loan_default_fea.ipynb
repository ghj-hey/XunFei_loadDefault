{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-04\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import entropy\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import datetime\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "nowtime = datetime.date.today()\n",
    "nowtime = str(nowtime)[-5:]\n",
    "print(nowtime)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def load_dataset(DATA_PATH):\n",
    "    train_label = pd.read_csv(DATA_PATH + 'train.csv')['loan_default']\n",
    "    train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "    train = reduce_mem_usage(train)\n",
    "    test = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "    test = reduce_mem_usage(test)\n",
    "    feats = [f for f in train.columns if f not in ['customer_id', 'loan_default']]\n",
    "    t_feats = [f for f in train.columns if f not in ['customer_id']]\n",
    "    train = train[t_feats]\n",
    "    test = test[feats]\n",
    "    print('train.shape', train.shape)\n",
    "    print('test.shape', test.shape)\n",
    "\n",
    "    return train_label, train, test\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.\n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum()\n",
    "    print('内存占用{:.2f} MB'.format(start_mem))\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum()\n",
    "    print('优化后内存为: {:.2f} MB'.format(end_mem))\n",
    "    print('内存使用减少 {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# 处理时间\n",
    "def transform_time(x):\n",
    "    day = int(x.split(' ')[0])\n",
    "    hour = int(x.split(' ')[2].split('.')[0].split(':')[0])\n",
    "    minute = int(x.split(' ')[2].split('.')[0].split(':')[1])\n",
    "    second = int(x.split(' ')[2].split('.')[0].split(':')[2])\n",
    "    return 86400 * day + 3600 * hour + 60 * minute + second\n",
    "\n",
    "\n",
    "def transform_day(date1):\n",
    "    date2 = \"2020-01-01\"\n",
    "    date1 = time.strptime(date1, \"%Y-%m-%d\")\n",
    "    date2 = time.strptime(date2, \"%Y-%m-%d\")\n",
    "\n",
    "    # 根据上面需要计算日期还是日期时间，来确定需要几个数组段。下标0表示年，小标1表示月，依次类推...\n",
    "    # date1=datetime.datetime(date1[0],date1[1],date1[2],date1[3],date1[4],date1[5])\n",
    "    # date2=datetime.datetime(date2[0],date2[1],date2[2],date2[3],date2[4],date2[5])\n",
    "    date1 = datetime.datetime(date1[0], date1[1], date1[2])\n",
    "    date2 = datetime.datetime(date2[0], date2[1], date2[2])\n",
    "    # 返回两个变量相差的值，就是相差天数\n",
    "    # print((date2 - date1).days)  # 将天数转成int型\n",
    "    return (date2 - date1).days\n",
    "\n",
    "\n",
    "# transform_day('2007-09-01')\n",
    "\n",
    "def labelEncoder_df(df, features):\n",
    "    for i in features:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        df[i] = encoder.fit_transform(df[i])\n",
    "\n",
    "\n",
    "\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    "        类别特征，列表或者字符串，用于编码的类别列名\n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    "        用于均值编码的拆分次数\n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    "        定义目标类型是回归变量还是分类变量\n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        \n",
    "         吸收观测值并输出先前权重的函数\n",
    "         传递字典时，将使用默认的指数衰减函数：\n",
    "         k：后验与前验平均加权所需的观察数\n",
    "         f：较大的f->较小的斜率\n",
    "        '''\n",
    "        >>>example:\n",
    "        mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['regionidcity',\n",
    "                          'regionidneighborhood', 'regionidzip'],\n",
    "                target_type='regression'\n",
    "                )\n",
    "\n",
    "        X = mean_encoder.fit_transform(X, pd.Series(y))\n",
    "        X_test = mean_encoder.transform(X_test)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        # 判断是否为字典类型\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        # 判断函数是否可以被调用\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "\n",
    "    #  返回函数的静态方法，该方法不强制要求传递参数\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "\n",
    "        # 定义回归和分类两种调用方法\n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}'.format(variable, target)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification 分类方法\n",
    "        else:\n",
    "            nf_name = '{}_pred'.format(variable)\n",
    "            X_train['pred_temp'] = y_train  # regression 回归方法\n",
    "        prior = X_train['pred_temp'].mean()\n",
    "        \n",
    "        col_avg_y = X_train.groupby(variable)['pred_temp'].agg(['mean','size']).rename(columns={'mean':'mean','size':'beta'})\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n",
    "        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}'.format(variable, target)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred'.format(variable)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradeTrans(x):\n",
    "    dict = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "    result = dict[x]\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def myEntro(x):\n",
    "    \"\"\"\n",
    "        calculate shanno ent of x\n",
    "    \"\"\"\n",
    "    x = np.array(x)\n",
    "    x_value_list = set([x[i] for i in range(x.shape[0])])\n",
    "    ent = 0.0\n",
    "    for x_value in x_value_list:\n",
    "        p = float(x[x == x_value].shape[0]) / x.shape[0]\n",
    "        logp = np.log2(p)\n",
    "        ent -= p * logp\n",
    "    #     print(x_value,p,logp)\n",
    "    # print(ent)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def myRms(records):\n",
    "    records = list(records)\n",
    "    \"\"\"\n",
    "    均方根值 反映的是有效值而不是平均值\n",
    "    \"\"\"\n",
    "    return np.math.sqrt(sum([x ** 2 for x in records]) / len(records))\n",
    "\n",
    "\n",
    "def myMode(x):\n",
    "    return np.mean(pd.Series.mode(x))\n",
    "\n",
    "\n",
    "def myQ25(x):\n",
    "    return x.quantile(0.25)\n",
    "\n",
    "\n",
    "def myQ75(x):\n",
    "    return x.quantile(0.75)\n",
    "\n",
    "def myQ10(x):\n",
    "    return x.quantile(0.1)\n",
    "    \n",
    "def myQ90(x):\n",
    "    return x.quantile(0.9)\n",
    "\n",
    "\n",
    "def myRange(x):\n",
    "    return pd.Series.max(x) - pd.Series.min(x)\n",
    "\n",
    "\n",
    "# 预处理\n",
    "def data_preprocess(DATA_PATH):\n",
    "    train_label, train, test = load_dataset(DATA_PATH=DATA_PATH)\n",
    "    # 拼接数据\n",
    "\n",
    "    data = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "    print('初始拼接后：', data.shape)\n",
    "    \n",
    "    n_feat = ['main_account_loan_no','main_account_active_loan_no','main_account_overdue_no','main_account_outstanding_loan',\n",
    "              'main_account_sanction_loan','main_account_disbursed_loan','main_account_inactive_loan_no', 'main_account_tenure',\n",
    "              'sub_account_loan_no', 'sub_account_active_loan_no','sub_account_outstanding_loan','sub_account_sanction_loan',\n",
    "              'sub_account_disbursed_loan','main_account_monthly_payment','sub_account_monthly_payment', 'sub_account_inactive_loan_no',\n",
    "              'sub_account_tenure', \n",
    "              'total_account_loan_no','total_inactive_loan_no','total_overdue_no','total_outstanding_loan','total_sanction_loan',\n",
    "              'total_disbursed_loan','total_monthly_payment',\n",
    "#               'disbursed_amount','asset_cost','branch_id','supplier_id','manufacturer_id','area_id','employee_code_id','mobileno_flag',\n",
    "#               'idcard_flag','Driving_flag','passport_flag','credit_score','last_six_month_new_loan_no','last_six_month_defaulted_no',\n",
    "#               'average_age','credit_history','enquirie_no','loan_to_asset_ratio','outstanding_disburse_ratio','disburse_to_sactioned_ratio',\n",
    "#               'active_to_inactive_act_ratio','year_of_birth','disbursed_date','Credit_level','employment_type','age',\n",
    "             ]\n",
    "   \n",
    "    nameList = ['min', 'max', 'sum', 'mean', 'median', 'skew', 'std', 'mode', 'range', 'Q25','Q75']\n",
    "    statList = ['min', 'max', 'sum', 'mean', 'median', 'skew', 'std', myMode, myRange, myQ25, myQ75]\n",
    "\n",
    "#     nameList = ['max', 'sum', 'mean', 'median', 'skew', 'std']\n",
    "#     statList = ['max', 'sum', 'mean', 'median', 'skew', 'std']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(nameList)):\n",
    "        data['n_feat_{}'.format(nameList[i])] = data[n_feat].agg(statList[i], axis=1)\n",
    "    print('n特征处理后：', data.shape)\n",
    "\n",
    "    # count编码\n",
    "    count_list = ['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age',\n",
    "                  'Driving_flag','passport_flag','employment_type']\n",
    "    data = count_coding(data, count_list)\n",
    "    print('count编码后：', data.shape)\n",
    "    ### 用数值特征对类别特征做统计刻画，随便挑了几个跟price相关性最高的匿名特征\n",
    "    cross_cat = ['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age',\n",
    "                  'Driving_flag','passport_flag','employment_type']\n",
    "    cross_num = [ 'disbursed_amount','asset_cost','credit_score','last_six_month_new_loan_no','last_six_month_defaulted_no',\n",
    "                  'average_age','credit_history','enquirie_no','loan_to_asset_ratio','outstanding_disburse_ratio',\n",
    "                  'disburse_to_sactioned_ratio','active_to_inactive_act_ratio','Credit_level',\n",
    "              \n",
    "#               'main_account_loan_no','main_account_active_loan_no','main_account_overdue_no','main_account_outstanding_loan',\n",
    "#               'main_account_sanction_loan','main_account_disbursed_loan','main_account_inactive_loan_no', 'main_account_tenure',\n",
    "#               'sub_account_loan_no', 'sub_account_active_loan_no','sub_account_outstanding_loan','sub_account_sanction_loan',\n",
    "#               'sub_account_disbursed_loan','main_account_monthly_payment','sub_account_monthly_payment', 'sub_account_inactive_loan_no',\n",
    "#               'sub_account_tenure', \n",
    "#               'total_account_loan_no','total_inactive_loan_no','total_overdue_no','total_outstanding_loan','total_sanction_loan',\n",
    "#               'total_disbursed_loan','total_monthly_payment',\n",
    "                ]\n",
    "\n",
    "\n",
    "    data = cross_cat_num(data, cross_num, cross_cat)  # 一阶交叉\n",
    "    print('一阶特征处理后：', data.shape)\n",
    "#     data = cross_qua_cat_num(data)  # 二阶交叉\n",
    "#     print('二阶特征处理后：', data.shape)\n",
    "\n",
    "\n",
    "#     data['grade'] = data['grade'].apply(lambda x: gradeTrans(x))\n",
    "#     data['subGrade'] = data['subGrade'].apply(lambda x: subGradeTrans(x))\n",
    "\n",
    "\n",
    "    print('预处理完毕', data.shape)\n",
    "\n",
    "    return data, train_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def kfold_stats_feature(train, test, feats, k):\n",
    "    folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=2021)  # 这里最好和后面模型的K折交叉验证保持一致\n",
    "\n",
    "    train['fold'] = None\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['loan_default'])):\n",
    "        train.loc[val_idx, 'fold'] = fold_\n",
    "\n",
    "    kfold_features = []\n",
    "    for feat in feats:\n",
    "        nums_columns = ['loan_default']\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            kfold_features.append(colname)\n",
    "            train[colname] = None\n",
    "            for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train['loan_default'])):\n",
    "                tmp_trn = train.iloc[trn_idx]\n",
    "                order_label = tmp_trn.groupby([feat])[f].mean()\n",
    "                tmp = train.loc[train.fold == fold_, [feat]]\n",
    "                train.loc[train.fold == fold_, colname] = tmp[feat].map(order_label)\n",
    "                # fillna\n",
    "                global_mean = train[f].mean()\n",
    "                train.loc[train.fold == fold_, colname] = train.loc[train.fold == fold_, colname].fillna(global_mean)\n",
    "            train[colname] = train[colname].astype(float)\n",
    "\n",
    "        for f in nums_columns:\n",
    "            colname = feat + '_' + f + '_kfold_mean'\n",
    "            test[colname] = None\n",
    "            order_label = train.groupby([feat])[f].mean()\n",
    "            test[colname] = test[feat].map(order_label)\n",
    "            # fillna\n",
    "            global_mean = train[f].mean()\n",
    "            test[colname] = test[colname].fillna(global_mean)\n",
    "            test[colname] = test[colname].astype(float)\n",
    "    del train['fold']\n",
    "    return train, test\n",
    "\n",
    "def GridSearch(clf, params, X, y):\n",
    "    cscv = GridSearchCV(clf, params, scoring='roc_auc', n_jobs=8, cv=10)\n",
    "    cscv.fit(X, y)\n",
    "    print(cscv.cv_results_)\n",
    "    print(cscv.best_params_)\n",
    "    print(cscv.best_score_)\n",
    "\n",
    "### count编码\n",
    "def count_coding(df, fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return (df)\n",
    "\n",
    "\n",
    "# 定义交叉特征统计\n",
    "def cross_cat_num(df, num_col, cat_col):\n",
    "    for f1 in tqdm(cat_col):\n",
    "        g = df.groupby(f1, as_index=False)\n",
    "        for f2 in tqdm(num_col):\n",
    "            feat = g[f2].agg({\n",
    "                '{}_{}_max'.format(f1, f2): 'max', '{}_{}_min'.format(f1, f2): 'min',\n",
    "                '{}_{}_median'.format(f1, f2): 'median',\n",
    "            })\n",
    "            df = df.merge(feat, on=f1, how='left')\n",
    "    return (df)\n",
    "\n",
    "\n",
    "def cross_qua_cat_num(df):\n",
    "    for f_pair in tqdm([\n",
    "        \n",
    "        ['branch_id','area_id'],['branch_id','employee_code_id'],['branch_id','employment_type'],\n",
    "        ['supplier_id','employment_type'],['supplier_id', 'Driving_flag'],['supplier_id','passport_flag'],\n",
    "        ['manufacturer_id','employment_type'],['manufacturer_id','year_of_birth'],['manufacturer_id','age'],\n",
    "        ['employee_code_id','area_id'],['employee_code_id','Driving_flag'],['employee_code_id','passport_flag'],\n",
    "        ['employee_code_id','employment_type']                     \n",
    "    ]):\n",
    "        ### 共现次数\n",
    "        df['_'.join(f_pair) + '_count'] = df.groupby(f_pair)['customer_id'].transform('count')\n",
    "        ### n unique、熵\n",
    "        df = df.merge(df.groupby(f_pair[0], as_index=False)[f_pair[1]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[0], f_pair[1]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[0], f_pair[1]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[0], how='left')\n",
    "        df = df.merge(df.groupby(f_pair[1], as_index=False)[f_pair[0]].agg({\n",
    "            '{}_{}_nunique'.format(f_pair[1], f_pair[0]): 'nunique',\n",
    "            '{}_{}_ent'.format(f_pair[1], f_pair[0]): lambda x: entropy(x.value_counts() / x.shape[0])\n",
    "        }), on=f_pair[1], how='left')\n",
    "        ### 比例偏好\n",
    "        df['{}_in_{}_prop'.format(f_pair[0], f_pair[1])] = df['_'.join(f_pair) + '_count'] / df[f_pair[1] + '_count']\n",
    "        df['{}_in_{}_prop'.format(f_pair[1], f_pair[0])] = df['_'.join(f_pair) + '_count'] / df[f_pair[0] + '_count']\n",
    "    return (df)\n",
    "\n",
    "\n",
    "### count编码\n",
    "def count_coding(df, fea_col):\n",
    "    for f in fea_col:\n",
    "        df[f + '_count'] = df[f].map(df[f].value_counts())\n",
    "    return (df)\n",
    "\n",
    "def gen_basicFea(data):\n",
    "    \n",
    "    data['new_loan_to_asset_ratio'] = data['disbursed_amount'] / (data['asset_cost'] * data['credit_score'])\n",
    "#     data['new_credit_ratio'] = (data['main_account_outstanding_loan'] + data['sub_account_outstanding_loan'])/\\\n",
    "#     (data['credit_history'] * data['credit_score'])\n",
    "\n",
    "#     data['new_payment_ration'] = (data['main_account_sanction_loan']+data['main_account_disbursed_loan']+data['sub_account_sanction_loan']+\\\n",
    "#     data['sub_account_disbursed_loan'])/(data['main_account_monthly_payment']+data['sub_account_monthly_payment'])\n",
    "\n",
    "#     for col in ['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age']:\n",
    "#         data['{}_count'.format(col)] = data.groupby([col])['customer_id'].transform('count')\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def plotroc(train_y, train_pred, test_y, val_pred):\n",
    "    lw = 2\n",
    "    ##train\n",
    "    fpr, tpr, thresholds = roc_curve(train_y.values, train_pred, pos_label=1.0)\n",
    "    train_auc_value = roc_auc_score(train_y.values, train_pred)\n",
    "    ##valid\n",
    "    fpr, tpr, thresholds = roc_curve(test_y.values, val_pred, pos_label=1.0)\n",
    "    valid_auc_value = roc_auc_score(test_y.values, val_pred)\n",
    "\n",
    "    return train_auc_value, valid_auc_value\n",
    "\n",
    "\n",
    "def lgb_model(train, target, test, k):\n",
    "\n",
    "#     saveFeature_df = pd.read_csv('../feature/lgb_importance.csv')\n",
    "#     saveFeature_list = list(saveFeature_df['Feature'][:290])\n",
    "    saveFeature_list=list(train.columns)\n",
    "    feats = [f for f in saveFeature_list if f not in ['customer_id', 'loan_default']]\n",
    "    feaNum = len(feats)\n",
    "    print('Current num of lgb features:', len(feats))\n",
    "\n",
    "    seeds = [2020,6666]\n",
    "    output_preds = 0\n",
    "    lgb_oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "    for seed in seeds:\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "        offline_score = []\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        params = {\n",
    "                    'boosting_type': 'gbdt','objective': 'binary','metric': 'auc','learning_rate': 0.01,\n",
    "                    'bagging_fraction': 1.0, 'bagging_freq': 44, 'feature_fraction': 0.5, \n",
    "                    'max_depth': 3,\n",
    "                    'min_child_weight': 10.0, 'min_data_in_leaf': 33, 'min_split_gain': 0.14174021024592806,\n",
    "                    'num_leaves': 29, 'reg_alpha': 7.588866417707964, 'reg_lambda': 10.0,\n",
    "                    'seed': seed,'n_jobs': -1,'verbose': -1,\n",
    "                  }\n",
    "        for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "            \n",
    "            train_y, test_y = target[train_index], target[test_index]\n",
    "            train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "            train_matrix = lgb.Dataset(train_X, label=train_y)\n",
    "            valid_matrix = lgb.Dataset(test_X, label=test_y)\n",
    "            test_matrix = test[feats]\n",
    "            watchlist = [train_matrix, valid_matrix]\n",
    "            \n",
    "            model = lgb.train(params, train_matrix, num_boost_round=20000, valid_sets=watchlist,\n",
    "                          verbose_eval=500, early_stopping_rounds=500)\n",
    "            \n",
    "            val_pred = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "            train_pred = model.predict(train_X, num_iteration=model.best_iteration)\n",
    "                                 \n",
    "            lgb_oof_probs[test_index] += val_pred / len(seeds)\n",
    "            # oof_probs[test_index] += val_pred\n",
    "            test_pred = model.predict(test_matrix, num_iteration=model.best_iteration,predict_disable_shape_check=True)\n",
    "#             test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "            # 绘制roc曲线\n",
    "            train_auc_value, valid_auc_value = plotroc(train_y, train_pred, test_y, val_pred)\n",
    "            print('lgb_train_auc:{},valid_auc{}'.format(train_auc_value, valid_auc_value))\n",
    "            offline_score.append(valid_auc_value)\n",
    "            print(offline_score)\n",
    "            output_preds += test_pred / k / len(seeds)\n",
    "            \n",
    "            fold_importance_df = pd.DataFrame()\n",
    "#             booster = model.booster_\n",
    "            fold_importance_df[\"Feature\"] = model.feature_name()\n",
    "            fold_importance_df[\"importance\"] = model.feature_importance(importance_type='split')\n",
    "            fold_importance_df[\"fold\"] = i + 1\n",
    "\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "\n",
    "        print('lgb_all_auc:', roc_auc_score(target.values, oof_probs))\n",
    "        print('OOF-MEAN-AUC lgb :%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "        feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "        feature_sorted.to_csv('../feature/lgb_importance.csv')\n",
    "        top_features = feature_sorted.index\n",
    "        print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "    return output_preds, lgb_oof_probs, np.mean(offline_score), feaNum\n",
    "\n",
    "def xgb_model(train, target, test, k):\n",
    "\n",
    "    saveFeature_list=list(train.columns)\n",
    "    feats = [f for f in saveFeature_list if f not in ['customer_id', 'loan_default']]\n",
    "    feaNum = len(feats)\n",
    "    print('Current num of xgb features:', len(feats))\n",
    "\n",
    "    seeds = [2020,6666]\n",
    "    output_preds = 0\n",
    "    xgb_oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "    for seed in seeds:\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "        offline_score = []\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        params = {'booster': 'gbtree',\n",
    "                  'objective': 'binary:logistic',\n",
    "                  'eval_metric': 'auc',\n",
    "                  'min_child_weight': 5,\n",
    "                  'max_depth': 8,\n",
    "                  'subsample': ss,\n",
    "                  'colsample_bytree': fs,\n",
    "                  'eta': 0.01,\n",
    "                  'seed': seed,\n",
    "                  'nthread': -1,\n",
    "                  'tree_method': 'gpu_hist'\n",
    "                  }\n",
    "        for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "            \n",
    "            train_y, test_y = target[train_index], target[test_index]\n",
    "            train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "            train_matrix = xgb.DMatrix(train_X, label=train_y, missing=np.nan)\n",
    "            valid_matrix = xgb.DMatrix(test_X, label=test_y, missing=np.nan)\n",
    "            test_matrix = xgb.DMatrix(test[feats], missing=np.nan)\n",
    "            watchlist = [(train_matrix, 'train'), (valid_matrix, 'eval')]\n",
    "            model = xgb.train(params, train_matrix, num_boost_round=20000, evals=watchlist, verbose_eval=100,\n",
    "                              early_stopping_rounds=500)\n",
    "            val_pred = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            train_pred = model.predict(train_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            xgb_oof_probs[test_index] += val_pred / len(seeds)\n",
    "            # oof_probs[test_index] += val_pred\n",
    "            test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "            # 绘制roc曲线\n",
    "            train_auc_value, valid_auc_value = plotroc(train_y, train_pred, test_y, val_pred)\n",
    "            print('xgb_train_auc:{},valid_auc{}'.format(train_auc_value, valid_auc_value))\n",
    "            offline_score.append(valid_auc_value)\n",
    "            print(offline_score)\n",
    "            output_preds += test_pred / k / len(seeds)\n",
    "            \n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"Feature\"] = model.get_fscore().keys()\n",
    "            fold_importance_df[\"importance\"] = model.get_fscore().values()\n",
    "            fold_importance_df[\"fold\"] = i + 1\n",
    "\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "\n",
    "        print('xgb_all_auc:', roc_auc_score(target.values, oof_probs))\n",
    "        print('OOF-MEAN-AUC xgb :%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "        feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "        feature_sorted.to_csv('../feature/xgb_importance.csv')\n",
    "        top_features = feature_sorted.index\n",
    "        print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "    return output_preds, xgb_oof_probs, np.mean(offline_score), feaNum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取数据...\n",
      "内存占用63600128.00 MB\n",
      "优化后内存为: 18900128.00 MB\n",
      "内存使用减少 70.3%\n",
      "内存占用12480128.00 MB\n",
      "优化后内存为: 3720128.00 MB\n",
      "内存使用减少 70.2%\n",
      "train.shape (150000, 52)\n",
      "test.shape (30000, 51)\n",
      "初始拼接后： (180000, 52)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\n",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n特征处理后： (180000, 63)\n",
      "count编码后： (180000, 73)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  8%|██████▍                                                                            | 1/13 [00:00<00:02,  4.89it/s]\u001b[A\n",
      " 23%|███████████████████▏                                                               | 3/13 [00:00<00:01,  5.99it/s]\u001b[A\n",
      " 38%|███████████████████████████████▉                                                   | 5/13 [00:00<00:01,  7.05it/s]\u001b[A\n",
      " 54%|████████████████████████████████████████████▋                                      | 7/13 [00:00<00:00,  7.83it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████▍                         | 9/13 [00:00<00:00,  8.55it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 11/13 [00:01<00:00,  9.18it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00, 10.18it/s]\u001b[A\n",
      " 10%|████████▎                                                                          | 1/10 [00:01<00:11,  1.28s/it]\n",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██████▍                                                                            | 1/13 [00:00<00:01,  9.37it/s]\u001b[A\n",
      " 15%|████████████▊                                                                      | 2/13 [00:00<00:01,  9.42it/s]\u001b[A\n",
      " 23%|███████████████████▏                                                               | 3/13 [00:00<00:01,  8.86it/s]\u001b[A\n",
      " 31%|█████████████████████████▌                                                         | 4/13 [00:00<00:01,  8.77it/s]\u001b[A\n",
      " 38%|███████████████████████████████▉                                                   | 5/13 [00:00<00:00,  8.87it/s]\u001b[A\n",
      " 46%|██████████████████████████████████████▎                                            | 6/13 [00:00<00:00,  8.55it/s]\u001b[A\n",
      " 54%|████████████████████████████████████████████▋                                      | 7/13 [00:00<00:00,  8.16it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████                                | 8/13 [00:00<00:00,  8.18it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████▍                         | 9/13 [00:01<00:00,  8.17it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████                   | 10/13 [00:01<00:00,  8.20it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 11/13 [00:01<00:00,  8.17it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 12/13 [00:01<00:00,  8.01it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  8.26it/s]\u001b[A\n",
      " 20%|████████████████▌                                                                  | 2/10 [00:02<00:10,  1.37s/it]\n",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██████▍                                                                            | 1/13 [00:00<00:01,  7.83it/s]\u001b[A\n",
      " 15%|████████████▊                                                                      | 2/13 [00:00<00:01,  7.55it/s]\u001b[A\n",
      " 23%|███████████████████▏                                                               | 3/13 [00:00<00:01,  7.53it/s]\u001b[A\n",
      " 31%|█████████████████████████▌                                                         | 4/13 [00:00<00:01,  7.48it/s]\u001b[A\n",
      " 38%|███████████████████████████████▉                                                   | 5/13 [00:00<00:01,  7.48it/s]\u001b[A\n",
      " 46%|██████████████████████████████████████▎                                            | 6/13 [00:00<00:00,  7.40it/s]\u001b[A\n",
      " 54%|████████████████████████████████████████████▋                                      | 7/13 [00:00<00:00,  7.34it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████                                | 8/13 [00:01<00:00,  7.19it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████▍                         | 9/13 [00:01<00:00,  7.15it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████                   | 10/13 [00:01<00:00,  6.88it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 11/13 [00:01<00:00,  6.86it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 12/13 [00:01<00:00,  6.83it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:01<00:00,  7.06it/s]\u001b[A\n",
      " 30%|████████████████████████▉                                                          | 3/10 [00:04<00:10,  1.51s/it]\n",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██████▍                                                                            | 1/13 [00:00<00:01,  7.06it/s]\u001b[A\n",
      " 15%|████████████▊                                                                      | 2/13 [00:00<00:01,  6.90it/s]\u001b[A\n",
      " 23%|███████████████████▏                                                               | 3/13 [00:00<00:01,  6.75it/s]\u001b[A\n",
      " 31%|█████████████████████████▌                                                         | 4/13 [00:00<00:01,  6.57it/s]\u001b[A\n",
      " 38%|███████████████████████████████▉                                                   | 5/13 [00:00<00:01,  6.42it/s]\u001b[A\n",
      " 46%|██████████████████████████████████████▎                                            | 6/13 [00:00<00:01,  6.26it/s]\u001b[A\n",
      " 54%|████████████████████████████████████████████▋                                      | 7/13 [00:01<00:00,  6.26it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████                                | 8/13 [00:01<00:00,  6.23it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████▍                         | 9/13 [00:01<00:00,  6.21it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████                   | 10/13 [00:01<00:00,  6.11it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 11/13 [00:01<00:00,  6.05it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 12/13 [00:01<00:00,  5.96it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  6.15it/s]\u001b[A\n",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:06<00:10,  1.70s/it]\n",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██████▍                                                                            | 1/13 [00:00<00:01,  6.04it/s]\u001b[A\n",
      " 15%|████████████▊                                                                      | 2/13 [00:00<00:01,  5.82it/s]\u001b[A\n",
      " 23%|███████████████████▏                                                               | 3/13 [00:00<00:01,  5.77it/s]\u001b[A\n",
      " 31%|█████████████████████████▌                                                         | 4/13 [00:00<00:01,  5.73it/s]\u001b[A\n",
      " 38%|███████████████████████████████▉                                                   | 5/13 [00:00<00:01,  5.61it/s]\u001b[A\n",
      " 46%|██████████████████████████████████████▎                                            | 6/13 [00:01<00:01,  5.52it/s]\u001b[A\n",
      " 54%|████████████████████████████████████████████▋                                      | 7/13 [00:01<00:01,  5.52it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████                                | 8/13 [00:01<00:00,  5.52it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████▍                         | 9/13 [00:01<00:00,  5.51it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████                   | 10/13 [00:01<00:00,  5.50it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 11/13 [00:01<00:00,  5.43it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 12/13 [00:02<00:00,  5.39it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  5.47it/s]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:09<00:09,  1.90s/it]\n",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██████▍                                                                            | 1/13 [00:00<00:02,  5.22it/s]\u001b[A\n",
      " 15%|████████████▊                                                                      | 2/13 [00:00<00:02,  5.15it/s]\u001b[A\n",
      " 23%|███████████████████▏                                                               | 3/13 [00:00<00:01,  5.10it/s]\u001b[A\n",
      " 31%|█████████████████████████▌                                                         | 4/13 [00:00<00:01,  4.95it/s]\u001b[A\n",
      " 38%|███████████████████████████████▉                                                   | 5/13 [00:01<00:01,  4.93it/s]\u001b[A\n",
      " 46%|██████████████████████████████████████▎                                            | 6/13 [00:01<00:01,  4.73it/s]\u001b[A\n",
      " 54%|████████████████████████████████████████████▋                                      | 7/13 [00:01<00:01,  4.72it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████                                | 8/13 [00:01<00:01,  4.67it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████▍                         | 9/13 [00:01<00:00,  4.60it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████                   | 10/13 [00:02<00:00,  4.61it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 11/13 [00:02<00:00,  4.58it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 12/13 [00:02<00:00,  4.57it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:02<00:00,  4.67it/s]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:12<00:08,  2.17s/it]\n",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██████▍                                                                            | 1/13 [00:00<00:02,  4.60it/s]\u001b[A\n",
      " 15%|████████████▊                                                                      | 2/13 [00:00<00:02,  4.53it/s]\u001b[A\n",
      " 23%|███████████████████▏                                                               | 3/13 [00:00<00:02,  4.50it/s]\u001b[A\n",
      " 31%|█████████████████████████▌                                                         | 4/13 [00:00<00:01,  4.51it/s]\u001b[A\n",
      " 38%|███████████████████████████████▉                                                   | 5/13 [00:01<00:01,  4.45it/s]\u001b[A\n",
      " 46%|██████████████████████████████████████▎                                            | 6/13 [00:01<00:01,  4.37it/s]\u001b[A\n",
      " 54%|████████████████████████████████████████████▋                                      | 7/13 [00:01<00:01,  4.36it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████                                | 8/13 [00:01<00:01,  4.35it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████▍                         | 9/13 [00:02<00:00,  4.30it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████                   | 10/13 [00:02<00:00,  4.29it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 11/13 [00:02<00:00,  4.20it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 12/13 [00:02<00:00,  4.20it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  4.28it/s]\u001b[A\n",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [00:15<00:07,  2.43s/it]\n",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██████▍                                                                            | 1/13 [00:00<00:02,  4.38it/s]\u001b[A\n",
      " 15%|████████████▊                                                                      | 2/13 [00:00<00:02,  4.22it/s]\u001b[A\n",
      " 23%|███████████████████▏                                                               | 3/13 [00:00<00:02,  4.10it/s]\u001b[A\n",
      " 31%|█████████████████████████▌                                                         | 4/13 [00:01<00:02,  4.04it/s]\u001b[A\n",
      " 38%|███████████████████████████████▉                                                   | 5/13 [00:01<00:01,  4.02it/s]\u001b[A\n",
      " 46%|██████████████████████████████████████▎                                            | 6/13 [00:01<00:01,  4.01it/s]\u001b[A\n",
      " 54%|████████████████████████████████████████████▋                                      | 7/13 [00:01<00:01,  3.95it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████                                | 8/13 [00:02<00:01,  3.97it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████▍                         | 9/13 [00:02<00:01,  3.96it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████                   | 10/13 [00:02<00:00,  3.92it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 11/13 [00:02<00:00,  3.93it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 12/13 [00:03<00:00,  3.90it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.93it/s]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [00:18<00:05,  2.70s/it]\n",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██████▍                                                                            | 1/13 [00:00<00:03,  3.98it/s]\u001b[A\n",
      " 15%|████████████▊                                                                      | 2/13 [00:00<00:02,  3.92it/s]\u001b[A\n",
      " 23%|███████████████████▏                                                               | 3/13 [00:00<00:02,  3.86it/s]\u001b[A\n",
      " 31%|█████████████████████████▌                                                         | 4/13 [00:01<00:02,  3.83it/s]\u001b[A\n",
      " 38%|███████████████████████████████▉                                                   | 5/13 [00:01<00:02,  3.76it/s]\u001b[A\n",
      " 46%|██████████████████████████████████████▎                                            | 6/13 [00:01<00:01,  3.77it/s]\u001b[A\n",
      " 54%|████████████████████████████████████████████▋                                      | 7/13 [00:01<00:01,  3.70it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████                                | 8/13 [00:02<00:01,  3.68it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████▍                         | 9/13 [00:02<00:01,  3.60it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████                   | 10/13 [00:02<00:00,  3.58it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 11/13 [00:03<00:00,  3.52it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 12/13 [00:03<00:00,  3.48it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.62it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [00:21<00:02,  2.97s/it]\n",
      "  0%|                                                                                           | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|██████▍                                                                            | 1/13 [00:00<00:03,  3.69it/s]\u001b[A\n",
      " 15%|████████████▊                                                                      | 2/13 [00:00<00:03,  3.58it/s]\u001b[A\n",
      " 23%|███████████████████▏                                                               | 3/13 [00:00<00:02,  3.56it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|█████████████████████████▌                                                         | 4/13 [00:01<00:02,  3.47it/s]\u001b[A\n",
      " 38%|███████████████████████████████▉                                                   | 5/13 [00:01<00:02,  3.45it/s]\u001b[A\n",
      " 46%|██████████████████████████████████████▎                                            | 6/13 [00:01<00:02,  3.41it/s]\u001b[A\n",
      " 54%|████████████████████████████████████████████▋                                      | 7/13 [00:02<00:01,  3.46it/s]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████                                | 8/13 [00:02<00:01,  3.38it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████▍                         | 9/13 [00:02<00:01,  3.32it/s]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████                   | 10/13 [00:02<00:00,  3.30it/s]\u001b[A\n",
      " 85%|█████████████████████████████████████████████████████████████████████▍            | 11/13 [00:03<00:00,  3.31it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▋      | 12/13 [00:03<00:00,  3.34it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  3.36it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:25<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一阶特征处理后： (180000, 463)\n",
      "预处理完毕 (180000, 463)\n",
      "开始特征工程...\n",
      "data.shape (180000, 464)\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '../data/'\n",
    "print('读取数据...')\n",
    "data, train_label = data_preprocess(DATA_PATH=DATA_PATH)\n",
    "\n",
    "print('开始特征工程...')\n",
    "data = gen_basicFea(data)\n",
    "\n",
    "\n",
    "print('data.shape', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始模型训练...\n",
      "num0:mean_encode train.shape (150000, 478) (30000, 478)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num1:target_encode train.shape (150000, 485) (30000, 485)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:13<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num2:target_encode train.shape (150000, 513) (30000, 513)\n",
      "输入数据维度： (150000, 506) (30000, 506)\n",
      "Current num of lgb features: 505\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.702177\tvalid_1's auc: 0.662789\n",
      "[1000]\ttraining's auc: 0.725869\tvalid_1's auc: 0.665903\n",
      "[1500]\ttraining's auc: 0.746329\tvalid_1's auc: 0.668254\n",
      "[2000]\ttraining's auc: 0.764208\tvalid_1's auc: 0.670289\n",
      "[2500]\ttraining's auc: 0.779795\tvalid_1's auc: 0.67134\n",
      "[3000]\ttraining's auc: 0.79466\tvalid_1's auc: 0.672238\n",
      "[3500]\ttraining's auc: 0.808179\tvalid_1's auc: 0.67298\n",
      "[4000]\ttraining's auc: 0.820807\tvalid_1's auc: 0.673897\n",
      "[4500]\ttraining's auc: 0.83245\tvalid_1's auc: 0.674679\n",
      "[5000]\ttraining's auc: 0.843002\tvalid_1's auc: 0.675538\n",
      "[5500]\ttraining's auc: 0.852817\tvalid_1's auc: 0.676171\n",
      "[6000]\ttraining's auc: 0.862141\tvalid_1's auc: 0.676849\n",
      "[6500]\ttraining's auc: 0.870556\tvalid_1's auc: 0.677233\n",
      "[7000]\ttraining's auc: 0.878495\tvalid_1's auc: 0.677485\n",
      "[7500]\ttraining's auc: 0.885917\tvalid_1's auc: 0.677954\n",
      "[8000]\ttraining's auc: 0.892716\tvalid_1's auc: 0.678279\n",
      "[8500]\ttraining's auc: 0.899092\tvalid_1's auc: 0.678573\n",
      "[9000]\ttraining's auc: 0.904824\tvalid_1's auc: 0.678983\n",
      "[9500]\ttraining's auc: 0.91014\tvalid_1's auc: 0.679196\n",
      "[10000]\ttraining's auc: 0.915189\tvalid_1's auc: 0.679624\n",
      "[10500]\ttraining's auc: 0.919698\tvalid_1's auc: 0.679842\n",
      "[11000]\ttraining's auc: 0.923782\tvalid_1's auc: 0.680165\n",
      "[11500]\ttraining's auc: 0.924478\tvalid_1's auc: 0.680316\n",
      "Early stopping, best iteration is:\n",
      "[11130]\ttraining's auc: 0.924478\tvalid_1's auc: 0.680316\n",
      "lgb_train_auc:0.9243981153296983,valid_auc0.680316163039817\n",
      "[0.680316163039817]\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.699834\tvalid_1's auc: 0.675124\n",
      "[1000]\ttraining's auc: 0.72326\tvalid_1's auc: 0.677642\n",
      "[1500]\ttraining's auc: 0.743209\tvalid_1's auc: 0.678869\n",
      "[2000]\ttraining's auc: 0.760897\tvalid_1's auc: 0.679816\n",
      "[2500]\ttraining's auc: 0.777122\tvalid_1's auc: 0.681227\n",
      "[3000]\ttraining's auc: 0.791956\tvalid_1's auc: 0.682117\n",
      "[3500]\ttraining's auc: 0.805788\tvalid_1's auc: 0.683073\n",
      "[4000]\ttraining's auc: 0.818512\tvalid_1's auc: 0.683902\n",
      "[4500]\ttraining's auc: 0.830459\tvalid_1's auc: 0.68442\n",
      "[5000]\ttraining's auc: 0.841446\tvalid_1's auc: 0.684917\n",
      "[5500]\ttraining's auc: 0.851286\tvalid_1's auc: 0.685582\n",
      "[6000]\ttraining's auc: 0.860457\tvalid_1's auc: 0.686366\n",
      "[6500]\ttraining's auc: 0.869058\tvalid_1's auc: 0.687125\n",
      "[7000]\ttraining's auc: 0.876971\tvalid_1's auc: 0.687796\n",
      "[7500]\ttraining's auc: 0.884247\tvalid_1's auc: 0.688353\n",
      "[8000]\ttraining's auc: 0.891251\tvalid_1's auc: 0.6887\n",
      "[8500]\ttraining's auc: 0.897414\tvalid_1's auc: 0.689082\n",
      "[9000]\ttraining's auc: 0.903442\tvalid_1's auc: 0.689462\n",
      "[9500]\ttraining's auc: 0.909145\tvalid_1's auc: 0.690141\n",
      "[10000]\ttraining's auc: 0.914425\tvalid_1's auc: 0.69076\n",
      "[10500]\ttraining's auc: 0.918865\tvalid_1's auc: 0.691503\n",
      "[11000]\ttraining's auc: 0.921925\tvalid_1's auc: 0.691629\n",
      "Early stopping, best iteration is:\n",
      "[10900]\ttraining's auc: 0.921883\tvalid_1's auc: 0.691636\n",
      "lgb_train_auc:0.921882552736834,valid_auc0.6916357529602714\n",
      "[0.680316163039817, 0.6916357529602714]\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.700473\tvalid_1's auc: 0.673834\n",
      "[1000]\ttraining's auc: 0.724156\tvalid_1's auc: 0.677322\n",
      "[1500]\ttraining's auc: 0.744232\tvalid_1's auc: 0.678521\n",
      "[2000]\ttraining's auc: 0.761888\tvalid_1's auc: 0.679493\n",
      "[2500]\ttraining's auc: 0.777998\tvalid_1's auc: 0.680549\n",
      "[3000]\ttraining's auc: 0.792915\tvalid_1's auc: 0.68103\n",
      "[3500]\ttraining's auc: 0.806309\tvalid_1's auc: 0.681642\n",
      "[4000]\ttraining's auc: 0.818799\tvalid_1's auc: 0.682325\n",
      "[4500]\ttraining's auc: 0.829948\tvalid_1's auc: 0.683034\n",
      "[5000]\ttraining's auc: 0.840487\tvalid_1's auc: 0.683502\n",
      "[5500]\ttraining's auc: 0.850755\tvalid_1's auc: 0.683903\n",
      "[6000]\ttraining's auc: 0.860114\tvalid_1's auc: 0.684522\n",
      "[6500]\ttraining's auc: 0.868993\tvalid_1's auc: 0.684999\n",
      "[7000]\ttraining's auc: 0.877203\tvalid_1's auc: 0.685394\n",
      "[7500]\ttraining's auc: 0.88461\tvalid_1's auc: 0.68564\n",
      "[8000]\ttraining's auc: 0.891401\tvalid_1's auc: 0.685987\n",
      "[8500]\ttraining's auc: 0.897837\tvalid_1's auc: 0.686363\n",
      "[9000]\ttraining's auc: 0.903432\tvalid_1's auc: 0.686768\n",
      "[9500]\ttraining's auc: 0.908908\tvalid_1's auc: 0.686932\n",
      "Early stopping, best iteration is:\n",
      "[9378]\ttraining's auc: 0.907611\tvalid_1's auc: 0.686977\n",
      "lgb_train_auc:0.9075750189272922,valid_auc0.6869774301876181\n",
      "[0.680316163039817, 0.6916357529602714, 0.6869774301876181]\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.701121\tvalid_1's auc: 0.667482\n",
      "[1000]\ttraining's auc: 0.725008\tvalid_1's auc: 0.670803\n",
      "[1500]\ttraining's auc: 0.745375\tvalid_1's auc: 0.672901\n",
      "[2000]\ttraining's auc: 0.763193\tvalid_1's auc: 0.674252\n",
      "[2500]\ttraining's auc: 0.779553\tvalid_1's auc: 0.675461\n",
      "[3000]\ttraining's auc: 0.794437\tvalid_1's auc: 0.67645\n",
      "[3500]\ttraining's auc: 0.807934\tvalid_1's auc: 0.677169\n",
      "[4000]\ttraining's auc: 0.820461\tvalid_1's auc: 0.677821\n",
      "[4500]\ttraining's auc: 0.831813\tvalid_1's auc: 0.678501\n",
      "[5000]\ttraining's auc: 0.842285\tvalid_1's auc: 0.678992\n",
      "[5500]\ttraining's auc: 0.852174\tvalid_1's auc: 0.679644\n",
      "[6000]\ttraining's auc: 0.861409\tvalid_1's auc: 0.680248\n",
      "[6500]\ttraining's auc: 0.869805\tvalid_1's auc: 0.680408\n",
      "[7000]\ttraining's auc: 0.877588\tvalid_1's auc: 0.680724\n",
      "[7500]\ttraining's auc: 0.885014\tvalid_1's auc: 0.681289\n",
      "[8000]\ttraining's auc: 0.891685\tvalid_1's auc: 0.681836\n",
      "[8500]\ttraining's auc: 0.897813\tvalid_1's auc: 0.681981\n",
      "[9000]\ttraining's auc: 0.903428\tvalid_1's auc: 0.682241\n",
      "[9500]\ttraining's auc: 0.908675\tvalid_1's auc: 0.682343\n",
      "[10000]\ttraining's auc: 0.913723\tvalid_1's auc: 0.682828\n",
      "[10500]\ttraining's auc: 0.918319\tvalid_1's auc: 0.683156\n",
      "[11000]\ttraining's auc: 0.922576\tvalid_1's auc: 0.683365\n",
      "[11500]\ttraining's auc: 0.926252\tvalid_1's auc: 0.683701\n",
      "[12000]\ttraining's auc: 0.927721\tvalid_1's auc: 0.684155\n",
      "Early stopping, best iteration is:\n",
      "[11757]\ttraining's auc: 0.927702\tvalid_1's auc: 0.684161\n",
      "lgb_train_auc:0.9276976220395636,valid_auc0.68416069787768\n",
      "[0.680316163039817, 0.6916357529602714, 0.6869774301876181, 0.68416069787768]\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.701467\tvalid_1's auc: 0.667897\n",
      "[1000]\ttraining's auc: 0.724833\tvalid_1's auc: 0.670787\n",
      "[1500]\ttraining's auc: 0.744791\tvalid_1's auc: 0.672959\n",
      "[2000]\ttraining's auc: 0.762119\tvalid_1's auc: 0.674581\n",
      "[2500]\ttraining's auc: 0.778126\tvalid_1's auc: 0.676057\n",
      "[3000]\ttraining's auc: 0.792736\tvalid_1's auc: 0.677602\n",
      "[3500]\ttraining's auc: 0.806026\tvalid_1's auc: 0.678717\n",
      "[4000]\ttraining's auc: 0.818448\tvalid_1's auc: 0.679987\n",
      "[4500]\ttraining's auc: 0.830266\tvalid_1's auc: 0.681053\n",
      "[5000]\ttraining's auc: 0.84093\tvalid_1's auc: 0.681981\n",
      "[5500]\ttraining's auc: 0.850734\tvalid_1's auc: 0.682737\n",
      "[6000]\ttraining's auc: 0.859917\tvalid_1's auc: 0.683575\n",
      "[6500]\ttraining's auc: 0.868718\tvalid_1's auc: 0.684365\n",
      "[7000]\ttraining's auc: 0.876359\tvalid_1's auc: 0.684936\n",
      "[7500]\ttraining's auc: 0.883517\tvalid_1's auc: 0.685541\n",
      "[8000]\ttraining's auc: 0.89047\tvalid_1's auc: 0.685914\n",
      "[8500]\ttraining's auc: 0.896748\tvalid_1's auc: 0.686372\n",
      "[9000]\ttraining's auc: 0.902522\tvalid_1's auc: 0.686834\n",
      "[9500]\ttraining's auc: 0.907839\tvalid_1's auc: 0.687377\n",
      "[10000]\ttraining's auc: 0.912902\tvalid_1's auc: 0.687855\n",
      "[10500]\ttraining's auc: 0.917606\tvalid_1's auc: 0.688387\n",
      "[11000]\ttraining's auc: 0.921599\tvalid_1's auc: 0.688856\n",
      "[11500]\ttraining's auc: 0.924475\tvalid_1's auc: 0.689672\n",
      "Early stopping, best iteration is:\n",
      "[11482]\ttraining's auc: 0.924435\tvalid_1's auc: 0.689682\n",
      "lgb_train_auc:0.9244142199202029,valid_auc0.6896764369254007\n",
      "[0.680316163039817, 0.6916357529602714, 0.6869774301876181, 0.68416069787768, 0.6896764369254007]\n",
      "lgb_all_auc: 0.5\n",
      "OOF-MEAN-AUC lgb :0.686553, OOF-STD-AUC:0.004010\n",
      "Feature\n",
      "employee_code_id_loan_default_kfold_mean            6661.6\n",
      "loan_to_asset_ratio                                 6550.8\n",
      "supplier_id_loan_default_kfold_mean                 5727.8\n",
      "employee_code_id_pred_0                             5453.2\n",
      "asset_cost                                          5425.6\n",
      "employee_code_id_target_skew                        5147.6\n",
      "disbursed_amount                                    5147.6\n",
      "employee_code_id_target_std                         4978.2\n",
      "supplier_id_pred_0                                  4585.2\n",
      "supplier_id_target_skew                             4584.4\n",
      "supplier_id_target_std                              4492.0\n",
      "employee_code_id_outstanding_disburse_ratio_max     4380.4\n",
      "employee_code_id_pred_1                             4189.6\n",
      "employee_code_id_loan_to_asset_ratio_max            4112.0\n",
      "employee_code_id_count                              4057.4\n",
      "employee_code_id_asset_cost_min                     3919.0\n",
      "supplier_id_pred_1                                  3853.2\n",
      "supplier_id_loan_to_asset_ratio_min                 3735.8\n",
      "employee_code_id_disbursed_amount_max               3702.8\n",
      "employee_code_id_disbursed_amount_median            3621.2\n",
      "employee_code_id_disbursed_amount_min               3594.6\n",
      "employee_code_id_loan_to_asset_ratio_median         3563.4\n",
      "employee_code_id_average_age_max                    3533.6\n",
      "employee_code_id_loan_to_asset_ratio_min            3441.4\n",
      "employee_code_id_asset_cost_max                     3426.6\n",
      "supplier_id_asset_cost_min                          3388.6\n",
      "employee_code_id_credit_history_max                 3355.6\n",
      "supplier_id_loan_to_asset_ratio_max                 3340.2\n",
      "year_of_birth_loan_default_kfold_mean               3138.4\n",
      "supplier_id_outstanding_disburse_ratio_max          3101.6\n",
      "supplier_id_disbursed_amount_min                    3085.2\n",
      "supplier_id_count                                   3071.2\n",
      "year_of_birth_pred_0                                3063.4\n",
      "supplier_id_loan_to_asset_ratio_median              3015.6\n",
      "branch_id_pred_0                                    2862.4\n",
      "branch_id_loan_default_kfold_mean                   2857.4\n",
      "supplier_id_disbursed_amount_median                 2834.2\n",
      "n_feat_skew                                         2817.6\n",
      "employee_code_id_disburse_to_sactioned_ratio_min    2797.6\n",
      "supplier_id_disbursed_amount_max                    2658.2\n",
      "supplier_id_credit_history_max                      2603.4\n",
      "year_of_birth_target_skew                           2509.6\n",
      "supplier_id_outstanding_disburse_ratio_min          2480.0\n",
      "credit_score                                        2470.8\n",
      "new_loan_to_asset_ratio                             2465.0\n",
      "employee_code_id_asset_cost_median                  2428.2\n",
      "supplier_id_asset_cost_median                       2387.8\n",
      "branch_id_target_skew                               2332.6\n",
      "year_of_birth_outstanding_disburse_ratio_min        2199.2\n",
      "supplier_id_asset_cost_max                          2138.2\n",
      "Name: importance, dtype: float64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.700161\tvalid_1's auc: 0.674363\n",
      "[1000]\ttraining's auc: 0.72357\tvalid_1's auc: 0.677943\n",
      "[1500]\ttraining's auc: 0.743683\tvalid_1's auc: 0.679695\n",
      "[2000]\ttraining's auc: 0.762281\tvalid_1's auc: 0.681442\n",
      "[2500]\ttraining's auc: 0.77854\tvalid_1's auc: 0.682418\n",
      "[3000]\ttraining's auc: 0.793237\tvalid_1's auc: 0.683219\n",
      "[3500]\ttraining's auc: 0.806903\tvalid_1's auc: 0.683856\n",
      "[4000]\ttraining's auc: 0.819142\tvalid_1's auc: 0.684678\n",
      "[4500]\ttraining's auc: 0.830622\tvalid_1's auc: 0.685345\n",
      "[5000]\ttraining's auc: 0.841007\tvalid_1's auc: 0.685722\n",
      "[5500]\ttraining's auc: 0.850686\tvalid_1's auc: 0.686385\n",
      "[6000]\ttraining's auc: 0.859839\tvalid_1's auc: 0.686946\n",
      "[6500]\ttraining's auc: 0.868114\tvalid_1's auc: 0.687413\n",
      "[7000]\ttraining's auc: 0.876051\tvalid_1's auc: 0.687638\n",
      "[7500]\ttraining's auc: 0.883473\tvalid_1's auc: 0.687923\n",
      "[8000]\ttraining's auc: 0.890274\tvalid_1's auc: 0.688129\n",
      "[8500]\ttraining's auc: 0.896593\tvalid_1's auc: 0.688665\n",
      "[9000]\ttraining's auc: 0.902636\tvalid_1's auc: 0.688941\n",
      "[9500]\ttraining's auc: 0.908298\tvalid_1's auc: 0.689096\n",
      "[10000]\ttraining's auc: 0.913188\tvalid_1's auc: 0.689496\n",
      "[10500]\ttraining's auc: 0.91769\tvalid_1's auc: 0.689825\n",
      "[11000]\ttraining's auc: 0.921695\tvalid_1's auc: 0.690124\n",
      "[11500]\ttraining's auc: 0.922197\tvalid_1's auc: 0.690149\n",
      "Early stopping, best iteration is:\n",
      "[11029]\ttraining's auc: 0.921908\tvalid_1's auc: 0.690161\n",
      "lgb_train_auc:0.9218173953478062,valid_auc0.690161009783314\n",
      "[0.690161009783314]\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.702124\tvalid_1's auc: 0.66593\n",
      "[1000]\ttraining's auc: 0.725726\tvalid_1's auc: 0.668588\n",
      "[1500]\ttraining's auc: 0.746158\tvalid_1's auc: 0.670746\n",
      "[2000]\ttraining's auc: 0.763944\tvalid_1's auc: 0.672098\n",
      "[2500]\ttraining's auc: 0.779807\tvalid_1's auc: 0.673245\n",
      "[3000]\ttraining's auc: 0.794291\tvalid_1's auc: 0.674289\n",
      "[3500]\ttraining's auc: 0.807699\tvalid_1's auc: 0.675291\n",
      "[4000]\ttraining's auc: 0.819988\tvalid_1's auc: 0.676301\n",
      "[4500]\ttraining's auc: 0.831429\tvalid_1's auc: 0.677116\n",
      "[5000]\ttraining's auc: 0.842236\tvalid_1's auc: 0.677755\n",
      "[5500]\ttraining's auc: 0.85205\tvalid_1's auc: 0.678434\n",
      "[6000]\ttraining's auc: 0.861141\tvalid_1's auc: 0.678989\n",
      "[6500]\ttraining's auc: 0.869339\tvalid_1's auc: 0.679492\n",
      "[7000]\ttraining's auc: 0.877213\tvalid_1's auc: 0.680004\n",
      "[7500]\ttraining's auc: 0.884294\tvalid_1's auc: 0.680292\n",
      "[8000]\ttraining's auc: 0.891115\tvalid_1's auc: 0.680787\n",
      "[8500]\ttraining's auc: 0.897264\tvalid_1's auc: 0.681036\n",
      "[9000]\ttraining's auc: 0.902934\tvalid_1's auc: 0.681585\n",
      "[9500]\ttraining's auc: 0.908583\tvalid_1's auc: 0.682033\n",
      "[10000]\ttraining's auc: 0.913511\tvalid_1's auc: 0.682291\n",
      "[10500]\ttraining's auc: 0.917963\tvalid_1's auc: 0.682592\n",
      "[11000]\ttraining's auc: 0.922182\tvalid_1's auc: 0.682934\n",
      "[11500]\ttraining's auc: 0.923984\tvalid_1's auc: 0.683341\n",
      "Early stopping, best iteration is:\n",
      "[11312]\ttraining's auc: 0.923973\tvalid_1's auc: 0.683349\n",
      "lgb_train_auc:0.9239430382316922,valid_auc0.683349297715316\n",
      "[0.690161009783314, 0.683349297715316]\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.70037\tvalid_1's auc: 0.672819\n",
      "[1000]\ttraining's auc: 0.723297\tvalid_1's auc: 0.67586\n",
      "[1500]\ttraining's auc: 0.744073\tvalid_1's auc: 0.678113\n",
      "[2000]\ttraining's auc: 0.761929\tvalid_1's auc: 0.67962\n",
      "[2500]\ttraining's auc: 0.778027\tvalid_1's auc: 0.681078\n",
      "[3000]\ttraining's auc: 0.793184\tvalid_1's auc: 0.682166\n",
      "[3500]\ttraining's auc: 0.80711\tvalid_1's auc: 0.683\n",
      "[4000]\ttraining's auc: 0.820122\tvalid_1's auc: 0.684047\n",
      "[4500]\ttraining's auc: 0.832023\tvalid_1's auc: 0.684687\n",
      "[5000]\ttraining's auc: 0.843115\tvalid_1's auc: 0.685387\n",
      "[5500]\ttraining's auc: 0.852957\tvalid_1's auc: 0.685967\n",
      "[6000]\ttraining's auc: 0.861914\tvalid_1's auc: 0.686529\n",
      "[6500]\ttraining's auc: 0.870548\tvalid_1's auc: 0.687273\n",
      "[7000]\ttraining's auc: 0.878356\tvalid_1's auc: 0.687838\n",
      "[7500]\ttraining's auc: 0.885446\tvalid_1's auc: 0.688152\n",
      "[8000]\ttraining's auc: 0.891929\tvalid_1's auc: 0.688256\n",
      "[8500]\ttraining's auc: 0.898326\tvalid_1's auc: 0.688494\n",
      "[9000]\ttraining's auc: 0.904066\tvalid_1's auc: 0.688609\n",
      "[9500]\ttraining's auc: 0.909391\tvalid_1's auc: 0.688693\n",
      "[10000]\ttraining's auc: 0.91437\tvalid_1's auc: 0.689229\n",
      "[10500]\ttraining's auc: 0.918881\tvalid_1's auc: 0.6898\n",
      "[11000]\ttraining's auc: 0.922991\tvalid_1's auc: 0.690504\n",
      "[11500]\ttraining's auc: 0.92692\tvalid_1's auc: 0.690783\n",
      "[12000]\ttraining's auc: 0.927557\tvalid_1's auc: 0.690866\n",
      "Early stopping, best iteration is:\n",
      "[11614]\ttraining's auc: 0.92754\tvalid_1's auc: 0.690873\n",
      "lgb_train_auc:0.9275219710536529,valid_auc0.690864777098507\n",
      "[0.690161009783314, 0.683349297715316, 0.690864777098507]\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.701091\tvalid_1's auc: 0.667333\n",
      "[1000]\ttraining's auc: 0.724715\tvalid_1's auc: 0.671127\n",
      "[1500]\ttraining's auc: 0.74453\tvalid_1's auc: 0.672723\n",
      "[2000]\ttraining's auc: 0.762384\tvalid_1's auc: 0.67389\n",
      "[2500]\ttraining's auc: 0.778612\tvalid_1's auc: 0.675186\n",
      "[3000]\ttraining's auc: 0.79345\tvalid_1's auc: 0.676161\n",
      "[3500]\ttraining's auc: 0.806971\tvalid_1's auc: 0.677191\n",
      "[4000]\ttraining's auc: 0.819501\tvalid_1's auc: 0.678122\n",
      "[4500]\ttraining's auc: 0.830789\tvalid_1's auc: 0.678387\n",
      "[5000]\ttraining's auc: 0.841509\tvalid_1's auc: 0.678633\n",
      "[5500]\ttraining's auc: 0.851471\tvalid_1's auc: 0.679127\n",
      "[6000]\ttraining's auc: 0.86055\tvalid_1's auc: 0.679589\n",
      "[6500]\ttraining's auc: 0.869206\tvalid_1's auc: 0.679951\n",
      "[7000]\ttraining's auc: 0.876961\tvalid_1's auc: 0.680202\n",
      "[7500]\ttraining's auc: 0.88411\tvalid_1's auc: 0.680468\n",
      "[8000]\ttraining's auc: 0.890842\tvalid_1's auc: 0.680821\n",
      "[8500]\ttraining's auc: 0.897393\tvalid_1's auc: 0.681196\n",
      "[9000]\ttraining's auc: 0.903285\tvalid_1's auc: 0.68146\n",
      "[9500]\ttraining's auc: 0.908808\tvalid_1's auc: 0.681686\n",
      "[10000]\ttraining's auc: 0.91382\tvalid_1's auc: 0.681757\n",
      "[10500]\ttraining's auc: 0.918852\tvalid_1's auc: 0.682238\n",
      "[11000]\ttraining's auc: 0.923213\tvalid_1's auc: 0.68256\n",
      "[11500]\ttraining's auc: 0.926718\tvalid_1's auc: 0.683135\n",
      "[12000]\ttraining's auc: 0.926919\tvalid_1's auc: 0.683208\n",
      "Early stopping, best iteration is:\n",
      "[11549]\ttraining's auc: 0.926919\tvalid_1's auc: 0.683208\n",
      "lgb_train_auc:0.926875474040531,valid_auc0.6832083199695\n",
      "[0.690161009783314, 0.683349297715316, 0.690864777098507, 0.6832083199695]\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[500]\ttraining's auc: 0.70165\tvalid_1's auc: 0.668343\n",
      "[1000]\ttraining's auc: 0.72496\tvalid_1's auc: 0.671232\n",
      "[1500]\ttraining's auc: 0.745548\tvalid_1's auc: 0.67297\n",
      "[2000]\ttraining's auc: 0.763739\tvalid_1's auc: 0.674078\n",
      "[2500]\ttraining's auc: 0.77982\tvalid_1's auc: 0.675053\n",
      "[3000]\ttraining's auc: 0.794399\tvalid_1's auc: 0.675981\n",
      "[3500]\ttraining's auc: 0.807592\tvalid_1's auc: 0.676764\n",
      "[4000]\ttraining's auc: 0.820186\tvalid_1's auc: 0.677469\n",
      "[4500]\ttraining's auc: 0.831782\tvalid_1's auc: 0.678193\n",
      "[5000]\ttraining's auc: 0.842481\tvalid_1's auc: 0.678978\n",
      "[5500]\ttraining's auc: 0.852307\tvalid_1's auc: 0.679626\n",
      "[6000]\ttraining's auc: 0.861731\tvalid_1's auc: 0.68022\n",
      "[6500]\ttraining's auc: 0.870185\tvalid_1's auc: 0.680589\n",
      "[7000]\ttraining's auc: 0.877922\tvalid_1's auc: 0.68099\n",
      "[7500]\ttraining's auc: 0.885391\tvalid_1's auc: 0.681345\n",
      "[8000]\ttraining's auc: 0.892428\tvalid_1's auc: 0.681859\n",
      "[8500]\ttraining's auc: 0.898848\tvalid_1's auc: 0.682323\n",
      "[9000]\ttraining's auc: 0.904757\tvalid_1's auc: 0.682759\n",
      "[9500]\ttraining's auc: 0.910093\tvalid_1's auc: 0.68321\n",
      "[10000]\ttraining's auc: 0.915074\tvalid_1's auc: 0.683691\n",
      "[10500]\ttraining's auc: 0.919637\tvalid_1's auc: 0.684094\n",
      "[11000]\ttraining's auc: 0.923718\tvalid_1's auc: 0.684387\n",
      "[11500]\ttraining's auc: 0.925448\tvalid_1's auc: 0.684665\n",
      "Early stopping, best iteration is:\n",
      "[11271]\ttraining's auc: 0.925447\tvalid_1's auc: 0.684667\n",
      "lgb_train_auc:0.9254398020295593,valid_auc0.6846654409282305\n",
      "[0.690161009783314, 0.683349297715316, 0.690864777098507, 0.6832083199695, 0.6846654409282305]\n",
      "lgb_all_auc: 0.5\n",
      "OOF-MEAN-AUC lgb :0.686450, OOF-STD-AUC:0.003364\n",
      "Feature\n",
      "employee_code_id_loan_default_kfold_mean            7009.0\n",
      "loan_to_asset_ratio                                 6825.4\n",
      "asset_cost                                          5880.4\n",
      "supplier_id_loan_default_kfold_mean                 5770.4\n",
      "disbursed_amount                                    5569.0\n",
      "employee_code_id_pred_0                             5449.8\n",
      "employee_code_id_target_skew                        5440.0\n",
      "employee_code_id_target_std                         5334.0\n",
      "supplier_id_target_skew                             4813.4\n",
      "supplier_id_target_std                              4718.6\n",
      "supplier_id_pred_0                                  4708.2\n",
      "employee_code_id_outstanding_disburse_ratio_max     4536.2\n",
      "employee_code_id_pred_1                             4486.8\n",
      "employee_code_id_count                              4362.4\n",
      "employee_code_id_loan_to_asset_ratio_max            4323.6\n",
      "employee_code_id_asset_cost_min                     4194.0\n",
      "supplier_id_pred_1                                  4114.4\n",
      "employee_code_id_disbursed_amount_min               3925.0\n",
      "employee_code_id_disbursed_amount_max               3859.4\n",
      "supplier_id_loan_to_asset_ratio_min                 3810.2\n",
      "employee_code_id_average_age_max                    3747.6\n",
      "employee_code_id_disbursed_amount_median            3712.4\n",
      "employee_code_id_loan_to_asset_ratio_median         3638.4\n",
      "employee_code_id_asset_cost_max                     3561.2\n",
      "supplier_id_loan_to_asset_ratio_max                 3538.4\n",
      "employee_code_id_credit_history_max                 3512.4\n",
      "supplier_id_asset_cost_min                          3461.8\n",
      "supplier_id_outstanding_disburse_ratio_max          3451.4\n",
      "employee_code_id_loan_to_asset_ratio_min            3444.2\n",
      "year_of_birth_loan_default_kfold_mean               3292.0\n",
      "supplier_id_count                                   3137.8\n",
      "branch_id_pred_0                                    3114.8\n",
      "year_of_birth_pred_0                                3114.0\n",
      "supplier_id_disbursed_amount_min                    3107.2\n",
      "supplier_id_loan_to_asset_ratio_median              3062.2\n",
      "employee_code_id_disburse_to_sactioned_ratio_min    2870.0\n",
      "n_feat_skew                                         2867.6\n",
      "branch_id_loan_default_kfold_mean                   2864.8\n",
      "supplier_id_disbursed_amount_median                 2857.8\n",
      "year_of_birth_target_skew                           2735.8\n",
      "supplier_id_credit_history_max                      2675.0\n",
      "new_loan_to_asset_ratio                             2653.8\n",
      "credit_score                                        2581.2\n",
      "employee_code_id_asset_cost_median                  2547.6\n",
      "supplier_id_outstanding_disburse_ratio_min          2506.8\n",
      "supplier_id_asset_cost_max                          2454.4\n",
      "supplier_id_asset_cost_median                       2424.0\n",
      "outstanding_disburse_ratio                          2365.2\n",
      "branch_id_target_skew                               2349.0\n",
      "year_of_birth_outstanding_disburse_ratio_min        2258.0\n",
      "Name: importance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "   \n",
    "    print('开始模型训练...')\n",
    "    train = data[~data['loan_default'].isnull()].copy()\n",
    "    target = train_label\n",
    "    test = data[data['loan_default'].isnull()].copy()\n",
    "\n",
    "    target_encode_cols = ['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age']\n",
    "\n",
    "    kflod_num = 5\n",
    "    ss = 0.8\n",
    "    fs = 0.4\n",
    "\n",
    "    class_list = ['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age']\n",
    "    MeanEnocodeFeature = class_list  # 声明需要平均数编码的特征\n",
    "    ME = MeanEncoder(MeanEnocodeFeature, target_type='classification')  # 声明平均数编码的类\n",
    "    train = ME.fit_transform(train, target)  # 对训练数据集的X和y进行拟合\n",
    "    # x_train_fav = ME.fit_transform(x_train,y_train_fav)#对训练数据集的X和y进行拟合\n",
    "    test = ME.transform(test)  # 对测试集进行编码\n",
    "    print('num0:mean_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "    train, test = kfold_stats_feature(train, test, target_encode_cols, kflod_num)\n",
    "    print('num1:target_encode train.shape', train.shape, test.shape)\n",
    "    ### target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "    enc_cols = []\n",
    "    stats_default_dict = {\n",
    "        'max': train['loan_default'].max(),\n",
    "        'min': train['loan_default'].min(),\n",
    "        'median': train['loan_default'].median(),\n",
    "        'mean': train['loan_default'].mean(),\n",
    "        'sum': train['loan_default'].sum(),\n",
    "        'std': train['loan_default'].std(),\n",
    "        'skew': train['loan_default'].skew(),\n",
    "        'kurt': train['loan_default'].kurt(),\n",
    "        'mad': train['loan_default'].mad()\n",
    "    }\n",
    "    ### 暂且选择这三种编码\n",
    "    enc_stats = ['max', 'min', 'skew', 'std']\n",
    "    skf = KFold(n_splits=kflod_num, shuffle=True, random_state=6666)\n",
    "    for f in tqdm(['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age']):\n",
    "        enc_dict = {}\n",
    "        for stat in enc_stats:\n",
    "            enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "            train['{}_target_{}'.format(f, stat)] = 0\n",
    "            test['{}_target_{}'.format(f, stat)] = 0\n",
    "            enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "        for i, (trn_idx, val_idx) in enumerate(skf.split(train, target)):\n",
    "            trn_x, val_x = train.iloc[trn_idx].reset_index(drop=True), train.iloc[val_idx].reset_index(drop=True)\n",
    "            enc_df = trn_x.groupby(f, as_index=False)['loan_default'].agg(enc_dict)\n",
    "            val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "            test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "            for stat in enc_stats:\n",
    "                val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                    stats_default_dict[stat])\n",
    "                test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                    stats_default_dict[stat])\n",
    "                train.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values\n",
    "                test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits\n",
    "\n",
    "    print('num2:target_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "    train.drop(['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age'], axis=1, inplace=True)\n",
    "    test.drop(['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age'], axis=1, inplace=True)\n",
    "    print('输入数据维度：', train.shape, test.shape)\n",
    "    \n",
    "    lgb_preds, lgb_oof, lgb_score, l_feaNum = lgb_model(train=train, target=target, test=test, k=kflod_num)\n",
    "#     xgb_preds, xgb_oof, xgb_score, x_feaNum = xgb_model(train=train, target=target, test=test, k=kflod_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体指标得分： 0.6901580870888346\n"
     ]
    }
   ],
   "source": [
    "lgb_score = round(lgb_score, 5)\n",
    "outpath = '../user_data/'\n",
    "\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "sample_submit = test[['customer_id']]\n",
    "test['loan_default']=-1\n",
    "# test['loan_default'] = 0.5 * lgb_preds + 0.5 * xgb_preds\n",
    "test['loan_default'] = lgb_preds\n",
    "\n",
    "\n",
    "test['loan_default'] = test['loan_default'].apply(lambda x: 1 if x > 0.24 else 0).values\n",
    "test_ = pd.read_csv('../data/test.csv')\n",
    "sample_submit = test_[['customer_id']]\n",
    "sample_submit['loan_default'] = test['loan_default']\n",
    "sample_submit.to_csv(outpath+'final_sub_lgb0.csv', index=False)\n",
    "\n",
    "train_df  = pd.read_csv('../data/train.csv')\n",
    "subVal_df = train_df[['customer_id']].copy()\n",
    "subVal_df['loan_default'] = lgb_oof\n",
    "\n",
    "all_auc_score = roc_auc_score(train_label, subVal_df['loan_default'])\n",
    "print('整体指标得分：', all_auc_score)\n",
    "all_auc_score = round(all_auc_score, 5)\n",
    "\n",
    "# subVal_df.to_csv(outpath+'force_alln_fea1_cross1D_lgb1Val0.csv',index=False)\n",
    "# sub_df.to_csv(\n",
    "#     outpath + str(all_auc_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_{}_xgb.csv'.format(ss, fs,\n",
    "#                                                                                                   kflod_num),\n",
    "#     index=False)\n",
    "# subVal_df.to_csv(\n",
    "#     outpath + str(all_auc_score) + '_' + str(feaNum) + '_' + nowtime + '_{}_{}_{}_subVal.csv'.format(ss, fs,\n",
    "#                                                                                                      kflod_num),\n",
    "#     index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(train, target, test, k):\n",
    "\n",
    "    \n",
    "    saveFeature_df = pd.read_csv('../feature/lgb_importance.csv')\n",
    "\n",
    "    \n",
    "    saveFeature_list = list(saveFeature_df['Feature'][:528])\n",
    "#     saveFeature_list=list(train.columns)\n",
    "    feats = [f for f in saveFeature_list if f not in ['customer_id', 'loan_default']]\n",
    "    feaNum = len(feats)\n",
    "    print('Current num of features:', len(feats))\n",
    "    \n",
    "\n",
    "\n",
    "    seeds = [2020,666666]\n",
    "    output_preds = 0\n",
    "    lgb_oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "    for seed in seeds:\n",
    "        folds = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        oof_probs = np.zeros(train.shape[0])\n",
    "\n",
    "        offline_score = []\n",
    "        feature_importance_df = pd.DataFrame()\n",
    "        params = {\n",
    "                    'boosting_type': 'gbdt','objective': 'binary','metric': 'auc','learning_rate': 0.01,\n",
    "                    'bagging_fraction': 1.0, 'bagging_freq': 44, 'feature_fraction': 0.5, 'max_depth': 6,\n",
    "                    'min_child_weight': 10.0, 'min_data_in_leaf': 33, 'min_split_gain': 0.14174021024592806,\n",
    "                    'num_leaves': 29, 'reg_alpha': 7.588866417707964, 'reg_lambda': 10.0,\n",
    "                    'seed': seed,'n_jobs': -1,'verbose': -1,\n",
    "                  }\n",
    "        for i, (train_index, test_index) in enumerate(folds.split(train, target)):\n",
    "            \n",
    "            train_y, test_y = target[train_index], target[test_index]\n",
    "            train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]\n",
    "            train_matrix = lgb.Dataset(train_X, label=train_y)\n",
    "            valid_matrix = lgb.Dataset(test_X, label=test_y)\n",
    "            test_matrix = test[feats]\n",
    "            watchlist = [train_matrix, valid_matrix]\n",
    "            \n",
    "            model = lgb.train(params, train_matrix, num_boost_round=20000, valid_sets=watchlist,\n",
    "                          verbose_eval=500, early_stopping_rounds=500)\n",
    "            \n",
    "            val_pred = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "            train_pred = model.predict(train_X, num_iteration=model.best_iteration)\n",
    "            \n",
    "            \n",
    "    \n",
    "     \n",
    "            lgb_oof_probs[test_index] += val_pred / len(seeds)\n",
    "            # oof_probs[test_index] += val_pred\n",
    "            test_pred = model.predict(test_matrix, num_iteration=model.best_iteration,predict_disable_shape_check=True)\n",
    "#             test_pred = model.predict(test_matrix, ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "            # 绘制roc曲线\n",
    "            train_auc_value, valid_auc_value = plotroc(train_y, train_pred, test_y, val_pred)\n",
    "            print('train_auc:{},valid_auc{}'.format(train_auc_value, valid_auc_value))\n",
    "            offline_score.append(valid_auc_value)\n",
    "            print(offline_score)\n",
    "            output_preds += test_pred / k / len(seeds)\n",
    "            \n",
    "\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "#             booster = model.booster_\n",
    "            fold_importance_df[\"Feature\"] = model.feature_name()\n",
    "            fold_importance_df[\"importance\"] = model.feature_importance(importance_type='split')\n",
    "            fold_importance_df[\"fold\"] = i + 1\n",
    "\n",
    "            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        print('all_auc:', roc_auc_score(target.values, oof_probs))\n",
    "        print('OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f' % (np.mean(offline_score), np.std(offline_score)))\n",
    "        feature_sorted = feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False)\n",
    "#         feature_sorted.to_csv('../feature/lgb_importance.csv')\n",
    "        top_features = feature_sorted.index\n",
    "        print(feature_importance_df.groupby(['Feature'])['importance'].mean().sort_values(ascending=False).head(50))\n",
    "    return output_preds, lgb_oof_probs, np.mean(offline_score), feaNum\n",
    "\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "print('读取数据...')\n",
    "data, train_label = data_preprocess(DATA_PATH=DATA_PATH)\n",
    "\n",
    "print('开始特征工程...')\n",
    "print('data.shape', data.shape)\n",
    "print('开始模型训练...')\n",
    "train = data[~data['loan_default'].isnull()].copy()\n",
    "target = train_label\n",
    "test = data[data['loan_default'].isnull()].copy()\n",
    "\n",
    "target_encode_cols = ['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age']\n",
    "\n",
    "kflod_num = 5\n",
    "ss = 0.8\n",
    "fs = 0.4\n",
    "\n",
    "class_list = ['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age']\n",
    "MeanEnocodeFeature = class_list  # 声明需要平均数编码的特征\n",
    "ME = MeanEncoder(MeanEnocodeFeature, target_type='classification')  # 声明平均数编码的类\n",
    "train = ME.fit_transform(train, target)  # 对训练数据集的X和y进行拟合\n",
    "# x_train_fav = ME.fit_transform(x_train,y_train_fav)#对训练数据集的X和y进行拟合\n",
    "test = ME.transform(test)  # 对测试集进行编码\n",
    "print('num0:mean_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "train, test = kfold_stats_feature(train, test, target_encode_cols, kflod_num)\n",
    "print('num1:target_encode train.shape', train.shape, test.shape)\n",
    "### target encoding目标编码，回归场景相对来说做目标编码的选择更多，不仅可以做均值编码，还可以做标准差编码、中位数编码等\n",
    "enc_cols = []\n",
    "stats_default_dict = {\n",
    "    'max': train['loan_default'].max(),\n",
    "    'min': train['loan_default'].min(),\n",
    "    'median': train['loan_default'].median(),\n",
    "    'mean': train['loan_default'].mean(),\n",
    "    'sum': train['loan_default'].sum(),\n",
    "    'std': train['loan_default'].std(),\n",
    "    'skew': train['loan_default'].skew(),\n",
    "    'kurt': train['loan_default'].kurt(),\n",
    "    'mad': train['loan_default'].mad()\n",
    "}\n",
    "### 暂且选择这三种编码\n",
    "enc_stats = ['max', 'min', 'skew', 'std']\n",
    "skf = KFold(n_splits=kflod_num, shuffle=True, random_state=6666)\n",
    "for f in tqdm(['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age']):\n",
    "    enc_dict = {}\n",
    "    for stat in enc_stats:\n",
    "        enc_dict['{}_target_{}'.format(f, stat)] = stat\n",
    "        train['{}_target_{}'.format(f, stat)] = 0\n",
    "        test['{}_target_{}'.format(f, stat)] = 0\n",
    "        enc_cols.append('{}_target_{}'.format(f, stat))\n",
    "    for i, (trn_idx, val_idx) in enumerate(skf.split(train, target)):\n",
    "        trn_x, val_x = train.iloc[trn_idx].reset_index(drop=True), train.iloc[val_idx].reset_index(drop=True)\n",
    "        enc_df = trn_x.groupby(f, as_index=False)['loan_default'].agg(enc_dict)\n",
    "        val_x = val_x[[f]].merge(enc_df, on=f, how='left')\n",
    "        test_x = test[[f]].merge(enc_df, on=f, how='left')\n",
    "        for stat in enc_stats:\n",
    "            val_x['{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                stats_default_dict[stat])\n",
    "            test_x['{}_target_{}'.format(f, stat)] = test_x['{}_target_{}'.format(f, stat)].fillna(\n",
    "                stats_default_dict[stat])\n",
    "            train.loc[val_idx, '{}_target_{}'.format(f, stat)] = val_x['{}_target_{}'.format(f, stat)].values\n",
    "            test['{}_target_{}'.format(f, stat)] += test_x['{}_target_{}'.format(f, stat)].values / skf.n_splits\n",
    "\n",
    "print('num2:target_encode train.shape', train.shape, test.shape)\n",
    "\n",
    "train.drop(['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age'], axis=1, inplace=True)\n",
    "test.drop(['branch_id', 'supplier_id', 'manufacturer_id', 'year_of_birth','area_id','employee_code_id','age'], axis=1, inplace=True)\n",
    "print('输入数据维度：', train.shape, test.shape)\n",
    "\n",
    "lgb_preds, lgb_oof, lgb_score, feaNum = lgb_model(train=train, target=target, test=test, k=kflod_num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
